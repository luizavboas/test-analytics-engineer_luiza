{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_learning_curve (estimator, X, y, n_splits, name):\n",
    "    '''\n",
    "        Plots the learning curve for the given classifier, using a k-fold cross validation\n",
    "        with n_splits.\n",
    "\n",
    "        Inputs:\n",
    "            estimator: scikit-learn classifier\n",
    "            X, y: dataset\n",
    "            n_splits: # of folders for k-fold cross validation\n",
    "            name: str: name of the classifier, to use in saving figure\n",
    "    '''\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import learning_curve\n",
    "\n",
    "    n_examples, train_scores, test_scores = learning_curve(estimator = estimator,\n",
    "                                                                X = X,\n",
    "                                                                y = y,\n",
    "                                                                train_sizes = np.linspace(0.1,1.0,20),\n",
    "                                                                cv = n_splits)\n",
    "\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(n_examples, train_mean,\n",
    "                        color='red', marker='*',\n",
    "                        markersize = 2, label='Mean training accuracy')\n",
    "\n",
    "    plt.plot(n_examples, test_mean,\n",
    "                color='blue', marker='s', \n",
    "                markersize = 3, label='Mean validation accuracy',)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Nº of Data points')\n",
    "    plt.title('Learning Curve '+name)\n",
    "    plt.ylim([0.0, 1.5])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('LearningCurve '+name+'test.png')\n",
    "    plt.show()\n",
    "\n",
    "def model_validation (estimator, X, y, param_name, param, n_splits, name):\n",
    "    '''\n",
    "        Plots the validation curve for the given classifier, using a k-fold cross validation\n",
    "        with n_splits over the parameter 'param' range.\n",
    "\n",
    "        Inputs:\n",
    "            estimator: scikit-learn classifier\n",
    "            param_name, param: estimator parameter to iterate over\n",
    "            X, y: dataset\n",
    "            n_splits: # of folders for k-fold cross validation\n",
    "            name: str: name of the classifier, to use in saving figure\n",
    "    '''\n",
    "\n",
    "    import matplotlib.pyplot as plt    \n",
    "    from sklearn.model_selection import validation_curve\n",
    "    train_scores, test_scores = validation_curve(estimator = estimator,\n",
    "                                                X = X,\n",
    "                                                y = y,\n",
    "                                                param_name = param_name,\n",
    "                                                param_range = param,\n",
    "                                                cv = n_splits)\n",
    "\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.plot(param, train_mean,\n",
    "            color='red', marker='*',\n",
    "            markersize = 2, label='Mean training accuracy')\n",
    "\n",
    "    plt.plot(param, test_mean,\n",
    "            color='blue', marker='s', \n",
    "            markersize = 3, label='Mean validation accuracy')\n",
    "\n",
    "    plt.grid()\n",
    "    plt.xscale('log')\n",
    "    plt.legend(loc='lower right')\n",
    "    xlabel_name = pd.DataFrame(np.array([['Logistic Regression','SVM','Decision Tree','Random Forest','SGD'],\n",
    "                                        ['C','C','Máx. Depth', 'Nº Estimators','Learning Rate']]).T, \n",
    "                                        columns = ['Classifier','Param'])\n",
    "\n",
    "    plt.xlabel(str((xlabel_name[xlabel_name['Classifier']==name]['Param']).values[0]))\n",
    "\n",
    "    plt.ylim([0.0, 1.2])\n",
    "    plt.title('Validation Curve '+name)\n",
    "    plt.ylabel('Accuracy')  \n",
    "    plt.savefig('ValidationCurve'+name+'test.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Loading modules \n",
    "\n",
    "# Preprocessing and splitting modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import (\n",
    "train_test_split, KFold, GridSearchCV, StratifiedShuffleSplit)\n",
    "\n",
    "# Learnign Algorithms Modules\n",
    "from sklearn.linear_model import (\n",
    "LogisticRegression, SGDClassifier, SGDRegressor)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiza\\AppData\\Local\\Temp\\ipykernel_9580\\691004612.py:19: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  ).mean()[[\"FinalPrice\"]].reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Folder path\n",
    "path =  \"../data/\"\n",
    "\n",
    "# Which values are considered NaN\n",
    "na_vls = ['#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', \n",
    "               '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', \n",
    "                'null', '...']\n",
    "\n",
    "# Load datasets\n",
    "df_products = pd.read_csv(path+\"New_ProdutosVarejos.csv\", na_values = na_vls)\n",
    "df_clts = pd.read_csv(path+\"Coletas.csv\", na_values = na_vls)\n",
    "\n",
    "# Transforming 'DateIns' column in a Pandas datetype time series\n",
    "df_clts[\"DateIns\"] = pd.to_datetime(df_clts[\"DateIns\"].values)\n",
    "\n",
    "df_join = df_products.merge(right = df_clts, how='inner',\\\n",
    "                    on = 'MasterKey_RetailerProduct').groupby(\\\n",
    "                        [\"Department\", \"Category\", \"Customer\", \"Product\",\"Specifics\", \"DateIns\",\"Brand\", \"EAN\", \"Retailer\"]\\\n",
    "                            ).mean()[[\"FinalPrice\"]].reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "537868     0\n",
      "537869     0\n",
      "537870     0\n",
      "537872     0\n",
      "537873     0\n",
      "          ..\n",
      "2848754    4\n",
      "2848756    4\n",
      "2848758    4\n",
      "2848771    4\n",
      "2848774    4\n",
      "Name: DateIns, Length: 87449, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[0 0 0 ... 1 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luiza\\Downloads\\test-analytics-engineer_luiza-1\\notebooks\\ml_training.ipynb Cell 3\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model \u001b[39m=\u001b[39m LinearRegression()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositive \u001b[39melse\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    649\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49maccept_sparse, y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m )\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype, only_non_negative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[39m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[39m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0 0 0 ... 1 1 1].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Customer Price Analysis\n",
    "selected_customer = 'Unilever'\n",
    "selected_brand = 'Dove'\n",
    "\n",
    "'''selected_product = 'Condicionador tresemmé hidratação profunda 400ml'''\n",
    "\n",
    "'''df_join = df_join[(df_join[\"Customer\"]==selected_customer)&\\\n",
    "                    (df_join[\"Product\"]==selected_product)]'''\n",
    "\n",
    "df_join = df_join[(df_join[\"Customer\"]==selected_customer)&\\\n",
    "                    (df_join[\"Brand\"]==selected_brand)]\n",
    "\n",
    "#pd.unique(df_join[\"Product\"])\n",
    "\n",
    "df_join.dropna(inplace = True)\n",
    "\n",
    "'''X = np.array([[df_join[\"DateIns\"].dt.dayofweek],\n",
    "    [df_join[\"DateIns\"].dt.day],\n",
    "    [df_join[\"DateIns\"].dt.month],\n",
    "    [df_join[\"DateIns\"].dt.year]]).T'''\n",
    "\n",
    "X = df_join[\"DateIns\"].dt.dayofweek\n",
    "print(X)\n",
    "print(type(X))\n",
    "\n",
    "y = df_join[\"FinalPrice\"].values\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Visualize the predictions and actual prices\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X_test, y_test, label='Actual Prices')\n",
    "plt.plot(X_test, y_pred, label='Predicted Prices', linestyle='dashed')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price Prediction using Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  DayOfWeek\n",
      "0       NaT\n",
      "1       NaT\n",
      "2       NaT\n",
      "3       NaT\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luiza\\Downloads\\test-analytics-engineer_luiza-1\\notebooks\\ml_training.ipynb Cell 4\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Make predictions\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:354\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    341\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_base.py:338\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    335\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    337\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(X, accept_sparse\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoo\u001b[39m\u001b[39m\"\u001b[39m], reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 338\u001b[0m \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef_\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mTypeError\u001b[0m: The DType <class 'numpy.dtype[datetime64]'> could not be promoted by <class 'numpy.dtype[float64]'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtype[datetime64]'>, <class 'numpy.dtype[float64]'>)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Datetime': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\n",
    "    'Price': [10, 15, 12, 18]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['Datetime'] = pd.to_datetime(df_join['DateIns'])\n",
    "\n",
    "# Feature engineering: Extract day of the week as a feature\n",
    "df['DayOfWeek'] = df['Datetime']\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df[['DayOfWeek']]\n",
    "\n",
    "print(X)\n",
    "print(type(X))\n",
    "\n",
    "y = df['Price']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Visualize the predictions and actual prices\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(X_test, y_test, label='Actual Prices')\n",
    "plt.plot(X_test, y_pred, label='Predicted Prices', linestyle='dashed')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Price Prediction using Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1.6860960e+18 1.6870464e+18 1.6831584e+18 ... 1.6877376e+18 1.6854048e+18\n 1.6833312e+18].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\luiza\\Downloads\\test-analytics-engineer_luiza-1\\notebooks\\ml_training.ipynb Cell 5\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size \u001b[39m=\u001b[39m test_size, random_state \u001b[39m=\u001b[39m random_state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Data Standardization\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m X_train_std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39;49mfit_transform(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m X_test_std \u001b[39m=\u001b[39m std\u001b[39m.\u001b[39mfit_transform(X_test, y_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/luiza/Downloads/test-analytics-engineer_luiza-1/notebooks/ml_training.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m## 3. Chosen estimators\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:881\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    878\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m--> 881\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    862\u001b[0m     X,\n\u001b[0;32m    863\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    864\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    865\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    866\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[0;32m    867\u001b[0m )\n\u001b[0;32m    868\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\Users\\luiza\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:902\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[39m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[39mif\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    903\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExpected 2D array, got 1D array instead:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39marray=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    905\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    906\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mif it contains a single sample.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[0;32m    907\u001b[0m         )\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m dtype_numeric \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mUSV\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    910\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdtype=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m'\u001b[39m\u001b[39m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    913\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1.6860960e+18 1.6870464e+18 1.6831584e+18 ... 1.6877376e+18 1.6854048e+18\n 1.6833312e+18].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Dataset dimensions:\n",
    "n_data = X.shape\n",
    "n_features = 1\n",
    "\n",
    "# Model Selection and Validation Parameters\n",
    "n_splits = 10   # --> # of folders for K-Fold Cross Validation step\n",
    "random_state = 42\n",
    "\n",
    "#nPCA = np.array(np.arange(0, n_features,2))\n",
    "#nPCA = np.array([3])\n",
    "nPCA = 1\n",
    "\n",
    "# Splitting and Standardization of training and test datasets\n",
    "std = StandardScaler()\n",
    "\n",
    "# Train test splitting\n",
    "test_size = 0.25\n",
    "train_size = 0.75\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = random_state)\n",
    "\n",
    "# Data Standardization\n",
    "X_train_std = std.fit_transform(X_train, y_train)\n",
    "X_test_std = std.fit_transform(X_test, y_test)\n",
    "\n",
    "## 3. Chosen estimators\n",
    "LR_estimator = make_pipeline(StandardScaler(), LogisticRegression(\n",
    "                    max_iter = 10000,\n",
    "                    random_state=random_state,))\n",
    "SVC_estimator =  make_pipeline(StandardScaler(), SVC())\n",
    "DT_estimator =   make_pipeline(StandardScaler(), DecisionTreeClassifier())\n",
    "RF_estimator =  make_pipeline(StandardScaler(), RandomForestClassifier())\n",
    "SGD_estimator = make_pipeline(StandardScaler(), SGDClassifier())\n",
    "\n",
    "# Grids for the estimators\n",
    "param = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "\n",
    "# Logistic Regression\n",
    "solvers = ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga']\n",
    "lr_grid = [{'logisticregression__solver': solvers, 'logisticregression__C':param[0:6]}]\n",
    "\n",
    "# SVM\n",
    "svc_grid = [{'svc__C': param, 'svc__kernel': ['linear']},\n",
    "{'svc__C': param, 'svc__gamma': param, 'svc__kernel': ['rbf']},\n",
    "{'svc__C': param, 'svc__kernel': ['poly'], 'svc__degree': [1,2,3,4]}]\n",
    "\n",
    "# Decision Tree and Random Forest\n",
    "max_depth = [1, 5, 10, 15, 20, 25, 30]\n",
    "n_estimators = [1, 5, 10, 15, 20, 25, 30]\n",
    "dt_grid = [{'decisiontreeclassifier__max_depth': max_depth}]\n",
    "rf_grid = {'randomforestclassifier__max_depth': max_depth, \n",
    "        'randomforestclassifier__n_estimators':n_estimators}\n",
    "\n",
    "#SGD parameters\n",
    "sgd_grid = {'sgdclassifier__learning_rate':['constant', \n",
    "            'optimal','invscaling', 'adaptive'], \n",
    "            'sgdclassifier__eta0':param[0:5]}\n",
    "\n",
    "\n",
    "## 4. Logistic Regression Validation\n",
    "lr_gs = GridSearchCV(estimator = LR_estimator,\n",
    "                    param_grid = lr_grid,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = n_splits)\n",
    "\n",
    "lr_gs = lr_gs.fit(X_train_std,y_train)\n",
    "    \n",
    "## 5. Support Vector Machine Validation\n",
    "svc_gs = GridSearchCV(estimator = SVC_estimator,\n",
    "                        param_grid = svc_grid,\n",
    "                        scoring = 'accuracy',\n",
    "                        cv = n_splits)\n",
    "\n",
    "svc_gs = svc_gs.fit(X_train_std, y_train)\n",
    "\n",
    "## 6. Decision Tree Validation\n",
    "dt_gs = GridSearchCV(DT_estimator,\n",
    "                    param_grid = dt_grid, \n",
    "                    scoring ='accuracy',\n",
    "                    cv = n_splits)\n",
    "dt_gs = dt_gs.fit(X_train_std, y_train)        \n",
    "\n",
    "## 7. Random Forest Validation\n",
    "rf_gs = GridSearchCV(estimator = RF_estimator,\n",
    "                    param_grid = rf_grid, \n",
    "                    scoring = 'accuracy',\n",
    "                    cv = n_splits)\n",
    "rf_gs = rf_gs.fit(X_train_std, y_train)\n",
    "\n",
    "## 7. SGD Validation\n",
    "sgd_gs = GridSearchCV(estimator = SGD_estimator,\n",
    "                    param_grid = sgd_grid, \n",
    "                    scoring = 'accuracy',\n",
    "                    cv = n_splits)\n",
    "sgd_gs = sgd_gs.fit(X_train_std, y_train)\n",
    "\n",
    "## 8. Classifier and hyperparameters choice\n",
    "print(\"Logistic Regression:\\nScore: \", lr_gs.best_score_,\"\\nBest Param: \", lr_gs.best_params_,\n",
    "\"\\nSupport Vector Machine:\\nScore: \", svc_gs.best_score_, \"\\nBest Param: \", svc_gs.best_params_,\n",
    "\"\\nDecision Tree:\\nScore: \", dt_gs.best_score_, \"\\nBest Param: \", dt_gs.best_params_,\n",
    "\"\\nRandom Forest:\\nScore: \", rf_gs.best_score_, \"\\nBest Param: \", rf_gs.best_params_,\n",
    "\"\\nSGD:\\nScore: \", sgd_gs.best_score_, \"\\nBest Param: \", sgd_gs.best_params_)\n",
    "\n",
    "\n",
    "## 9. Final Accuracy\n",
    "LR_estimator = lr_gs.best_estimator_\n",
    "model_learning_curve(LR_estimator, X_test_std, y_test, \n",
    "                    n_splits,\n",
    "                    'Logistic Regression')\n",
    "\n",
    "SVC_estimator = svc_gs.best_estimator_\n",
    "model_learning_curve(SVC_estimator, X_test_std, y_test, \n",
    "                     n_splits,'SVM')\n",
    "\n",
    "dt_estimator = dt_gs.best_estimator_\n",
    "model_learning_curve(DT_estimator, X_test_std, y_test,\n",
    "                    n_splits, \n",
    "                    'Decision Tree')   \n",
    "\n",
    "RF_estimator = rf_gs.best_estimator_\n",
    "model_learning_curve(RF_estimator, X_test_std, y_test, 10, 'Random Forest')\n",
    "                           \n",
    "SGD_estimator = sgd_gs.best_estimator_\n",
    "model_learning_curve(SGD_estimator, X_train_std, y_train, n_splits, 'SGD')\n",
    "\n",
    "\n",
    "print('Scores:',\n",
    "'\\nLR:', LR_estimator.score(X_test_std, y_test), \n",
    "'\\nSVM:', SVC_estimator.score(X_test_std, y_test), \n",
    "'\\nDT:', dt_estimator.score(X_test_std, y_test),  \n",
    "'\\nRF:', RF_estimator.score(X_test_std, y_test),\n",
    "'\\nSGD:', SGD_estimator.score(X_test_std, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
